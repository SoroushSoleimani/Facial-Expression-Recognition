{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4a774d-ca66-47f1-86a9-70e366522774",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759377e-c128-409b-a92e-58dfa7402ea7",
   "metadata": {},
   "source": [
    "## Environment Setup & Library Configuration\n",
    "\n",
    "This block serves as the foundation for the entire analysis notebook. Its main goal is to load the essential \"toolkit\" required to process facial images and analyze the dataset structure before any actual coding begins.\n",
    "\n",
    "It imports libraries for three specific purposes:\n",
    "\n",
    "-    __Data Handling:__ pandas and numpy are imported to manage the metadata (CSV files) and perform numerical calculations.\n",
    "\n",
    "-    __Image Processing:__ cv2 (OpenCV) and PIL are included to load, resize, and convert the actual image files.\n",
    "\n",
    "-    __Advanced Visualization:__ It includes matplotlib and seaborn for standard charts, and TSNE specifically for the advanced dimensionality reduction plot (to visualize how similar different emotions look).\n",
    "\n",
    "Finally, the last two lines set the global design style for the report, ensuring that all charts generated later have a consistent size (12Ã—6) and a clean \"whitegrid\" background for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562729c-22e0-41c8-91be-386041bfad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np   \n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567ded7-4be3-41de-9c10-45799d108a10",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Inspection\n",
    "\n",
    "The primary purpose of this code is to load the dataset into memory and perform a sanity check to ensure the data is structured correctly before analysis begins. It verifies that the file is readable and contains the expected number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece337f8-f48e-41e2-9841-425b5fd92ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../CSV files/dataset_cleaned.csv')\n",
    "\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c71d49-5c06-4151-b511-841365381140",
   "metadata": {},
   "source": [
    "## Class Distribution Analysis\n",
    "\n",
    "The main objective of this section is to visualize the balance of the dataset. It checks how many images exist for each emotion category (e.g., happy, angry, sad) to identify if any class is significantly underrepresented, which could lead to model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7a0b2-298c-4d1d-ac88-0b82399a446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.countplot(data=df, x='label', hue='label', legend=False, order=df['label'].value_counts().index, palette='viridis')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')This code counts the occurrences of each emotion and calculates their percentages to check for class imbalance. Its purpose is to ensure the dataset provides enough examples for every category so the model doesn't become biased toward a single expression.\n",
    "\n",
    "plt.title('Distribution of Facial Expressions')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d77fce-b9c4-463f-b96b-649257458fa1",
   "metadata": {},
   "source": [
    "__Output Analysis:__ The chart reveals the distribution across all 7 emotions. If one bar is much shorter than others (e.g., \"disgust\" having very few samples compared to \"happy\"), it indicates a class imbalance problem that might require techniques like data augmentation or class weighting during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966fd03-5708-400d-be7c-1ff0ca8dc951",
   "metadata": {},
   "source": [
    "## Percentage of each emotion\n",
    "\n",
    "This code counts the occurrences of each emotion and calculates their percentages to check for class imbalance. Its purpose is to ensure the dataset provides enough examples for every category so the model doesn't become biased toward a single expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c661d1-400d-4d98-b1b6-44d178f05e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "label_counts = df['label'].value_counts()\n",
    "plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\n",
    "plt.title('Percentage of Each Emotion', fontsize=16)\n",
    "plt.axis('equal') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea6d15d-c8c6-4565-b867-60ee53fa7f0d",
   "metadata": {},
   "source": [
    "__Output Analysis:__ The output shows that the data is well-distributed according to the percentage of each label. This balance means the model can learn the features of every emotion equally, leading to more reliable and fair performance across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b59f7e8-148d-46e1-90a7-a1b04d491066",
   "metadata": {},
   "source": [
    "## Visualizing Sample Images\n",
    "\n",
    "The main objective of this code is visual verification. Before training any AI model, you must confirm that the images are loading correctly and that the labels match the content (e.g., ensuring a file labeled \"happy\" actually shows a smiling face)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aaf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"../data\"\n",
    "unique_labels = df['label'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(unique_labels), figsize=(20, 5))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    sample_row = df[df['label'] == label].sample(1).iloc[0]\n",
    "    image_name = sample_row['filename']\n",
    "    image_path = os.path.join(IMAGE_DIR, image_name)\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(label)\n",
    "        axes[i].axis('off')\n",
    "    except Exception:\n",
    "        axes[i].text(0.5, 0.5, \"Not Found\", ha='center')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbe1f91-b64a-489f-a91e-edd68fa09e04",
   "metadata": {},
   "source": [
    "## Visualizing Feature Separability with t-SNE\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "-    __Data Sampling:__ It selects a subset of 1,000 images from the dataset. Processing every single image would be computationally heavy, so this sample provides a representative \"snapshot\" of the data.\n",
    "\n",
    "-    __Preprocessing:__ The code loops through the sampled images, converts them to grayscale, and resizes them to a uniform 48x48 resolution. It then flattens each image into a 1D array of pixels (2,304 features) so the mathematical model can process them.\n",
    "\n",
    "-    __t-SNE Transformation:__ It applies the t-SNE algorithm to reduce those 2,304 pixel dimensions down to just two (Dimension 1 and Dimension 2). t-SNE works by keeping similar images close together and pushing dissimilar images apart.\n",
    "\n",
    "-    __Visualization:__ Finally, it creates a scatter plot where each point represents an image, colored according to its emotion label.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The goal is to see how \"separable\" the emotions are based on raw pixel values. It helps us understand if certain emotions (like \"Angry\" and \"Disgust\") look very similar to the computer, which would make them harder for the AI to distinguish later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3a813-0a5e-4272-a87f-78fbb216c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_samples = min(n_samples, len(df))\n",
    "\n",
    "subset_df = df.sample(n_samples, random_state=42)\n",
    "image_data = []\n",
    "labels = []\n",
    "\n",
    "print(\"Processing images for t-SNE...\")\n",
    "\n",
    "for index, row in subset_df.iterrows():\n",
    "    path = os.path.join(IMAGE_DIR, row['filename'])\n",
    "    try:\n",
    "        img = cv2.imread(path, 0)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (48, 48))\n",
    "            image_data.append(img.flatten())\n",
    "            labels.append(row['label'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X = np.array(image_data)\n",
    "\n",
    "if len(X) > 0:\n",
    "    perp = min(30, len(X) - 1)\n",
    "    \n",
    "    if perp < 5:\n",
    "        print(\"Not enough data points for t-SNE!\")\n",
    "    else:\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=perp)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette='viridis', alpha=0.7)\n",
    "        plt.title(f't-SNE Projection of {len(X)} Facial Expressions', fontsize=15)\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Error: No images loaded. Please check 'IMAGE_DIR' path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffbdd3-869e-4745-9895-e33eca0781bd",
   "metadata": {},
   "source": [
    "### Conclusions from the Output\n",
    "\n",
    "-    __Cluster Formation:__ If you see distinct groups (clusters) of the same color, it means the raw pixels clearly differentiate those emotions.\n",
    "\n",
    "-    __Overlap Analysis:__ In facial recognition, you will often see significant overlap between classes. This indicates that raw pixel intensity alone isn't enough to tell emotions apart, justifying the need for a more complex Deep Learning model (like a CNN).\n",
    "\n",
    "-    __Data Integrity:__ If the points are completely mixed with no structure, it suggests high noise in the data or that the facial expressions are very subtle across different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73f147",
   "metadata": {},
   "source": [
    "## Edge Detection\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "-    __Automated Sampling:__ The code identifies all unique emotions (labels) in the dataset and selects one random sample image for each category.\n",
    "\n",
    "* **Dual Visualization:** It creates a grid with two rows:\n",
    "    * **Row 1 (Original):** Displays the raw grayscale facial images to provide a direct look at the data the model will \"see.\"\n",
    "    * **Row 2 (Canny Edge Detection):** Applies the Canny Edge Detection algorithm to the same images. This mathematical operation identifies areas with sharp changes in intensity, effectively highlighting the outlines of facial features like eyes, eyebrows, and mouths.\n",
    "\n",
    "-    __Format:__ It uses `plt.subplots` to align them perfectly for a side-by-side comparison across all emotion classes.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This section is crucial for understanding feature extraction. Facial expressions are primarily defined by the movement of specific muscles (the \"edges\" of the mouth or eyes). By visualizing the edges, we can verify if the important structural information of an emotion is preserved after filtering out background noise and lighting variations. It helps confirm that the images are clear enough for a model to recognize specific facial contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d2712-9ce2-425b-a974-00aaf6f529a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = df['label'].unique()\n",
    "\n",
    "fig, axes = plt.subplots(2, len(unique_labels), figsize=(20, 6))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    sample_row = df[df['label'] == label].sample(1).iloc[0]\n",
    "    path = os.path.join(IMAGE_DIR, sample_row['filename'])\n",
    "    \n",
    "    img = cv2.imread(path, 0)\n",
    "    \n",
    "    if img is not None:\n",
    "        axes[0, i].imshow(img, cmap='gray')\n",
    "        axes[0, i].set_title(f\"Original: {label}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        edges = cv2.Canny(img, 100, 200)\n",
    "        axes[1, i].imshow(edges, cmap='gray')\n",
    "        axes[1, i].set_title(f\"Edges: {label}\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904045ec",
   "metadata": {},
   "source": [
    "### Conclusions from the Output\n",
    "\n",
    "-    __Feature Prominence:__ In the \"Edges\" row, if the contours of the lips and eyes are sharp and distinct, it indicates that the dataset is of high quality for training a Convolutional Neural Network (CNN).\n",
    "\n",
    "-    __Noise Assessment:__ If the edge-detected images are filled with random \"speckles\" or background lines, it suggests the images might need further denoising or tighter cropping around the face.\n",
    "\n",
    "-    __Conclusion:__ The output confirms that the defining characteristics of each expression (like the curve of a smile or the furrow of a brow) are mathematically detectable, which is a strong indicator that the model will be able to extract these patterns during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4600fd-777a-41ac-abfa-e87413d7fc55",
   "metadata": {},
   "source": [
    "## Analyzing Image Brightness Distribution\n",
    "\n",
    "### Code Explanation\n",
    "\n",
    "-    __Sampling:__ Similar to the previous step, it takes a random sample of 1,000 images to ensure the analysis is statistically significant without being slow.\n",
    "\n",
    "-    __Brightness Calculation:__ The code iterates through each image path, loads the image in grayscale, and calculates the mean() of all pixel values. This average represents the overall \"brightness\" of that specific image (on a scale of 0 to 255).\n",
    "\n",
    "-    __Box Plot Visualization:__ It uses a Box Plot (grouped by emotion labels) to show the distribution of these brightness values. A box plot is ideal here because it shows the median, the range (whiskers), and any outliers (extreme dark or bright images).\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The goal is to determine if the lighting conditions are consistent across different emotion categories. In a robust dataset, \"Happy\" images shouldn't be significantly brighter than \"Sad\" images just because of the camera settings. If one category is much darker than others, the model might accidentally learn to associate \"darkness\" with that emotion rather than the actual facial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808d96c-4355-49bb-ad88-8732dbc45df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "brightness_data = []\n",
    "\n",
    "subset_df = df.sample(1000, random_state=42)\n",
    "\n",
    "for index, row in subset_df.iterrows():\n",
    "    path = os.path.join(IMAGE_DIR, row['filename'])\n",
    "    try:\n",
    "        img = cv2.imread(path, 0)\n",
    "        if img is not None:\n",
    "            avg_brightness = img.mean()\n",
    "            brightness_data.append({'label': row['label'], 'brightness': avg_brightness})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "brightness_df = pd.DataFrame(brightness_data)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=brightness_df, x='label', y='brightness', hue='label', legend=False, palette='coolwarm')\n",
    "plt.title('Distribution of Image Brightness per Emotion', fontsize=15)\n",
    "plt.ylabel('Average Pixel Intensity (0-255)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed7bfeb-983a-41cd-8e49-c48d74fe3fb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusions from the Output\n",
    "\n",
    "-    __Consistency:__ If the boxes for all emotions are at a similar height, it indicates that the lighting is well-balanced across the dataset.\n",
    "\n",
    "-    __Data Quality:__ Significant differences in brightness between classes suggest a need for Histogram Equalization or other normalization techniques during the preprocessing phase to prevent the model from becoming biased by lighting conditions.\n",
    "\n",
    "-    __Conclusion:__ This analysis ensures that the model focuses on facial expressions rather than environmental factors like room lighting or camera exposure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
